{"date": "20240907-091611", "backend": "vllm", "model_id": "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8", "tokenizer_id": "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8", "best_of": 1, "use_beam_search": false, "num_prompts": 1280, "request_rate": "inf", "duration": 4.659598083992023, "completed": 3, "total_input_tokens": 1232, "total_output_tokens": 0, "request_throughput": 0.6438323533324588, "input_throughput": 264.4004864351964, "output_throughput": 0.0, "mean_ttft_ms": 0.0, "median_ttft_ms": 0.0, "std_ttft_ms": 0.0, "p99_ttft_ms": 0.0, "mean_tpot_ms": 0.0, "median_tpot_ms": 0.0, "std_tpot_ms": 0.0, "p99_tpot_ms": 0.0, "mean_itl_ms": 0.0, "median_itl_ms": 0.0, "std_itl_ms": 0.0, "p99_itl_ms": 0.0, "mean_latency_ms": 8.370099370313255, "median_latency_ms": 0.0, "std_latency_ms": 172.78021072801081, "p99_latency_ms": 0.0, "input_lens": [13, 25, 27, 10, 329, 29, 399, 328, 4, 205, 16, 372, 403, 9, 12, 770, 227, 8, 14, 390, 659, 59, 76, 744, 619, 279, 344, 9, 10, 45, 11, 4, 80, 768, 317, 40, 619, 8, 298, 16, 17, 696, 575, 11, 7, 281, 12, 147, 481, 770, 336, 6, 331, 9, 45, 222, 802, 481, 93, 641, 22, 15, 16, 77, 32, 689, 178, 9, 628, 480, 42, 336, 24, 515, 761, 112, 113, 348, 82, 467, 245, 11, 24, 343, 13, 387, 258, 32, 28, 225, 248, 35, 133, 98, 27, 23, 58, 363, 203, 140, 140, 230, 310, 521, 768, 317, 298, 336, 491, 19, 10, 59, 48, 20, 20, 54, 107, 53, 72, 18, 10, 768, 7, 933, 239, 277, 26, 35, 462, 386, 25, 23, 17, 20, 44, 661, 631, 349, 53, 71, 37, 10, 21, 37, 496, 8, 9, 353, 10, 595, 5, 49, 286, 305, 56, 43, 52, 310, 35, 7, 7, 238, 10, 143, 660, 191, 72, 39, 258, 312, 49, 9, 15, 561, 76, 561, 501, 8, 66, 15, 303, 19, 455, 27, 143, 688, 9, 31, 361, 21, 117, 8, 12, 775, 338, 176, 349, 166, 419, 401, 307, 169, 647, 21, 9, 34, 12, 37, 45, 154, 8, 10, 18, 253, 486, 611, 96, 19, 62, 529, 15, 30, 10, 445, 680, 762, 441, 77, 12, 276, 178, 22, 159, 95, 366, 4, 7, 770, 8, 45, 480, 90, 32, 329, 768, 72, 86, 53, 83, 419, 44, 14, 56, 28, 707, 235, 27, 668, 17, 427, 44, 370, 8, 674, 455, 185, 68, 19, 132, 304, 9, 34, 39, 54, 768, 661, 404, 240, 25, 95, 60, 24, 12, 85, 66, 62, 462, 22, 241, 25, 32, 13, 175, 31, 342, 43, 339, 144, 233, 16, 10, 11, 269, 587, 8, 233, 10, 48, 9, 335, 22, 61, 29, 692, 7, 332, 512, 13, 128, 24, 24, 12, 205, 361, 384, 301, 7, 13, 612, 74, 34, 16, 14, 23, 93, 11, 630, 13, 526, 7, 22, 476, 415, 299, 17, 327, 11, 624, 504, 17, 21, 29, 264, 63, 58, 6, 4, 33, 183, 9, 369, 26, 76, 38, 14, 15, 273, 534, 17, 25, 281, 29, 246, 4, 19, 16, 7, 46, 709, 439, 10, 47, 257, 289, 19, 116, 7, 302, 248, 234, 16, 527, 9, 714, 33, 24, 245, 328, 13, 109, 267, 8, 4, 10, 14, 364, 577, 325, 50, 22, 189, 11, 78, 489, 319, 51, 4, 111, 830, 14, 385, 12, 181, 400, 16, 366, 37, 34, 41, 768, 18, 432, 553, 25, 32, 237, 748, 5, 366, 303, 225, 24, 705, 132, 466, 422, 526, 779, 471, 756, 197, 20, 9, 315, 29, 11, 192, 413, 683, 9, 295, 6, 426, 11, 16, 294, 40, 770, 6, 77, 77, 29, 9, 12, 57, 44, 19, 15, 13, 211, 197, 54, 36, 50, 176, 11, 13, 475, 138, 364, 769, 5, 209, 14, 331, 13, 125, 15, 770, 573, 11, 47, 156, 338, 471, 5, 22, 49, 768, 7, 458, 217, 139, 260, 362, 82, 197, 19, 65, 12, 209, 13, 561, 217, 316, 22, 8, 15, 9, 39, 175, 7, 16, 44, 18, 35, 670, 17, 12, 30, 224, 24, 641, 621, 13, 11, 73, 690, 77, 13, 547, 652, 123, 115, 6, 24, 18, 29, 379, 18, 13, 499, 49, 36, 210, 588, 19, 551, 599, 445, 770, 7, 444, 355, 34, 675, 28, 703, 49, 9, 770, 161, 11, 13, 10, 73, 836, 7, 46, 157, 116, 623, 316, 51, 21, 231, 71, 27, 234, 25, 373, 10, 498, 9, 9, 93, 86, 470, 33, 44, 239, 158, 8, 14, 324, 450, 79, 336, 282, 7, 65, 156, 83, 400, 771, 552, 102, 238, 472, 20, 211, 28, 305, 612, 167, 6, 14, 15, 660, 12, 10, 14, 761, 34, 22, 73, 59, 9, 23, 33, 17, 773, 4, 221, 21, 13, 4, 136, 31, 503, 247, 67, 386, 124, 17, 66, 734, 250, 265, 476, 11, 127, 392, 9, 9, 83, 15, 15, 772, 526, 16, 534, 582, 573, 12, 184, 18, 257, 261, 7, 457, 24, 327, 20, 720, 740, 157, 607, 180, 282, 515, 7, 11, 28, 253, 29, 21, 28, 15, 716, 7, 124, 18, 249, 27, 134, 18, 15, 229, 452, 216, 22, 21, 57, 44, 5, 17, 608, 46, 18, 8, 346, 14, 23, 245, 24, 25, 82, 650, 46, 11, 63, 697, 402, 14, 325, 509, 60, 24, 551, 606, 32, 13, 220, 349, 13, 18, 125, 154, 10, 8, 9, 8, 26, 56, 8, 247, 370, 769, 478, 192, 190, 563, 11, 118, 625, 15, 16, 60, 308, 769, 475, 229, 770, 20, 110, 327, 7, 27, 505, 618, 14, 782, 357, 67, 16, 770, 978, 261, 20, 49, 24, 398, 462, 14, 15, 401, 10, 769, 16, 23, 29, 291, 192, 125, 36, 288, 24, 19, 43, 155, 37, 647, 355, 358, 246, 10, 15, 78, 325, 11, 10, 364, 768, 378, 6, 42, 764, 761, 328, 318, 697, 770, 845, 28, 211, 486, 123, 306, 317, 19, 419, 46, 282, 118, 767, 163, 280, 28, 757, 767, 526, 270, 23, 497, 700, 5, 425, 18, 151, 102, 268, 48, 265, 318, 156, 5, 498, 17, 218, 27, 77, 357, 240, 560, 546, 331, 10, 11, 113, 935, 349, 16, 89, 11, 315, 452, 580, 768, 482, 335, 15, 31, 409, 335, 530, 614, 6, 6, 85, 770, 7, 578, 142, 5, 15, 18, 997, 14, 13, 585, 33, 8, 40, 23, 24, 369, 226, 15, 16, 16, 671, 262, 82, 16, 362, 345, 92, 180, 372, 376, 697, 48, 681, 34, 13, 35, 799, 729, 33, 20, 520, 9, 24, 15, 357, 9, 579, 534, 14, 44, 104, 53, 259, 44, 28, 33, 532, 305, 368, 9, 41, 768, 393, 67, 40, 153, 14, 155, 105, 597, 15, 282, 31, 20, 14, 892, 642, 765, 259, 749, 37, 5, 101, 558, 23, 327, 308, 15, 719, 477, 15, 170, 216, 24, 303, 22, 10, 6, 19, 324, 728, 39, 767, 17, 240, 318, 366, 18, 199, 769, 279, 320, 60, 533, 495, 25, 469, 907, 22, 352, 770, 7, 8, 771, 20, 11, 494, 49, 318, 768, 532, 6, 768, 13, 16, 481, 11, 25, 711, 31, 776, 401, 31, 9, 103, 18, 427, 305, 11, 766, 39, 517, 256, 67, 10, 14, 854, 66, 21, 14, 23, 562, 326, 593, 406, 9, 32, 122, 9, 201, 59, 162, 406, 34, 396, 8, 441, 20, 85, 6, 362, 212, 621, 58, 9, 41, 308, 322, 274, 16, 163, 127, 16, 41, 27, 263, 217, 19, 528, 41, 222, 770, 233, 415, 119, 495, 54, 10, 26, 663, 203, 39, 629, 95, 267, 156, 766, 180, 554, 106, 395, 5, 240, 14, 170, 309, 262, 329, 13, 62, 50, 383, 19, 229, 383, 302, 35, 8, 11, 358, 456, 11, 519, 394, 707, 4, 502, 20, 27, 8, 123, 32, 102, 566, 418, 435, 92, 96, 769, 552, 54, 8, 41, 497, 80, 12, 12, 260, 43, 512, 16, 70, 339, 155, 411, 18, 137, 12, 395, 762, 62, 164, 15, 733, 353, 293, 441, 457, 490, 10, 8, 13, 15, 454, 185, 420, 607, 44, 670, 64, 8, 11, 770, 355, 493, 354, 436, 519, 317, 305, 15, 8, 76, 493, 462, 88, 171, 85, 157, 14, 16, 15, 32, 14, 378, 143, 663, 501, 404, 224, 252, 110, 97, 602, 11, 67, 203, 8, 374, 529, 161, 205, 483, 356, 612, 23, 260, 720, 330, 16, 85, 79, 72, 464, 421, 14, 36, 291, 540, 243, 346, 250, 281, 539, 196, 11, 59, 51, 6], "output_lens": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "ttfts": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.4077026459999615, 3.407703593999031, 3.4075127999967663, 3.4074411789915757, 3.40740253101103, 3.40728439000668, 3.4072059110039845, 3.407506850999198, 3.4074660089972895, 3.4074172069958877, 3.407382723991759, 0.0, 3.651257703997544, 3.6512547059974167, 3.65116498999123, 3.6510729889996583, 3.650988266002969, 0.0, 3.652715278993128, 3.653921919001732, 0.0, 3.653740227993694, 3.653675123001449, 3.653619794000406, 3.653556122997543, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "itls": [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], "generated_texts": ["", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", ""], "errors": ["Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n"]}