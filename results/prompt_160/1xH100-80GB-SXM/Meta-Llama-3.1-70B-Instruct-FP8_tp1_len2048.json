{"date": "20240907-085519", "backend": "vllm", "model_id": "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8", "tokenizer_id": "neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8", "best_of": 1, "use_beam_search": false, "num_prompts": 160, "request_rate": "inf", "duration": 5.350801691005472, "completed": 21, "total_input_tokens": 5692, "total_output_tokens": 668, "request_throughput": 3.9246455414896677, "input_throughput": 1063.765829626628, "output_throughput": 124.84110579595705, "mean_ttft_ms": 1072.0405963331273, "median_ttft_ms": 1383.0872329999693, "std_ttft_ms": 718.2617980547631, "p99_ttft_ms": 2039.9837625969667, "mean_tpot_ms": 60.33605768206023, "median_tpot_ms": 48.00837165881107, "std_tpot_ms": 41.61744037181456, "p99_tpot_ms": 191.98615442253254, "mean_itl_ms": 403.68351317600764, "median_itl_ms": 362.0698349986924, "std_itl_ms": 105.98284562173917, "p99_itl_ms": 638.1866384910245, "mean_latency_ms": 375.99623993737623, "median_latency_ms": 0.0, "std_latency_ms": 1092.828466183204, "p99_latency_ms": 5249.594227158523, "input_lens": [13, 25, 27, 10, 329, 29, 399, 328, 4, 205, 16, 372, 403, 9, 12, 770, 227, 8, 14, 390, 659, 59, 76, 744, 619, 279, 344, 9, 10, 45, 11, 4, 80, 768, 317, 40, 619, 8, 298, 16, 17, 696, 575, 11, 7, 281, 12, 147, 481, 770, 336, 6, 331, 9, 45, 222, 802, 481, 93, 641, 22, 15, 16, 77, 32, 689, 178, 9, 628, 480, 42, 336, 24, 515, 761, 112, 113, 348, 82, 467, 245, 11, 24, 343, 13, 387, 258, 32, 28, 225, 248, 35, 133, 98, 27, 23, 58, 363, 203, 140, 140, 230, 310, 521, 768, 317, 298, 336, 491, 19, 10, 59, 48, 20, 20, 54, 107, 53, 72, 18, 10, 768, 7, 933, 239, 277, 26, 35, 462, 386, 25, 23, 17, 20, 44, 661, 631, 349, 53, 71, 37, 10, 21, 37, 496, 8, 9, 353, 10, 595, 5, 49, 286, 305, 56, 43, 52, 310, 35, 7], "output_lens": [0, 0, 92, 0, 101, 0, 0, 0, 6, 78, 0, 0, 0, 45, 0, 9, 0, 0, 0, 37, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 36, 26, 0, 0, 0, 4, 0, 0, 63, 24, 0, 0, 0, 0, 0, 99, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "ttfts": [1.4023863850015914, 1.402136349992361, 1.4020419550070073, 1.4019581000029575, 1.401696771994466, 1.4016053499944974, 0.0, 0.0, 1.4012682790053077, 1.405914295988623, 1.4058528870082228, 0.0, 1.4057354099932127, 1.4054609190061456, 1.4053996500006178, 2.0400794169981964, 1.4052717309969012, 1.4052059410023503, 1.4053476780100027, 2.039601144992048, 2.039539473989862, 2.039537395001389, 2.0393607930018334, 2.864934147000895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0809763919969555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0801763369963737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3838486750028096, 1.3838600239978405, 0.0, 1.3837066940031946, 1.0787732879980467, 1.0786785560048884, 0.0, 1.3829141769965645, 1.3830872329999693, 1.3830432920076419, 1.3826476550020743, 1.382578573000501, 1.382779848005157, 1.3827578789932886, 0.0, 1.3825441489898367, 1.3822092019981937, 1.3823670179990586, 1.3823011300119106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "itls": [[0.6381903569999849, 0.387787177998689, 0.4376885880046757, 0.3361133449943736, 0.4301726630073972, 0.33435257099336013, 0.3268008830054896, 0.32485723300487734, 0.324881180000375, 0.32491874998959247], [0.638184313007514, 0.38779429299756885, 0.4377057230012724, 0.3360794030013494, 0.4301737169880653, 0.33436872900347225, 0.3268114999955287, 0.3248479820031207, 0.32488304800062906, 0.3249170680064708], [0.6381829430029029, 0.3877935759956017, 0.4377073270006804, 0.3360765950055793, 0.43017878600221593, 0.3343645139975706, 0.3268073779909173, 0.3253972420061473, 0.32434046499838587, 0.31750147900311276], [0.6381846750009572, 0.38779487599094864, 0.43770707699877676, 0.33670505600457545, 0.4295493940007873, 0.33436475999769755, 0.32680974500544835, 0.32540315999358427, 0.32432989501103293, 0.3248526349925669], [0.6381966299959458, 0.3885199240030488, 0.43696704400645103, 0.33671401599713136, 0.43025067799317185, 0.3342451540083857, 0.3262176680000266, 0.3254031459946418, 0.3248933400027454, 0.3198231050046161], [0.6381792070023948, 0.38853922999987844, 0.43695703300181776, 0.3367074300040258, 0.43026833099429496, 0.33423887100070715, 0.32620791999215726, 0.32540536900341976, 0.32490048599720467, 0.3242175090126693], [], [], [0.33121936599491164], [0.6334307639917824, 0.38853156300319824, 0.4376676209940342, 0.335994703011238, 0.43027073598932475, 0.33423558600770775, 0.32621425499382894, 0.3168164170056116], [0.6334394250006881, 0.3885384270106442, 0.43766553699970245, 0.33598678099224344, 0.4302715950034326, 0.33423480700002983, 0.3262138629943365, 0.3253388590092072, 0.3248956339957658, 0.32422240999585483], [], [0.6334437250043266, 0.388533584002289, 0.43766927599790506, 0.3359871240099892, 0.4302179219957907, 0.3342205099907005, 0.32621311000548303, 0.32534036099968944, 0.32489487499697134, 0.32423302000097465], [0.6334345070063137, 0.38853558799019083, 0.43768853100482374, 0.3359654960077023, 0.31614642099884804], [0.6334412159922067, 0.3885297829983756, 0.43766940000932664, 0.33599524399323855, 0.43020348899881355, 0.3342314439942129, 0.3262134840042563, 0.3253396700019948, 0.3248969609994674, 0.324221695002052], [0.32881674901000224], [0.6334446010005195, 0.3885316270025214, 0.4376817390002543, 0.3359801409969805, 0.43020453400094993, 0.3342274859896861, 0.3262312230071984, 0.32532930199522525, 0.32489002399961464, 0.32422693600528874], [0.6334516939969035, 0.38853029999881983, 0.4376815360010369, 0.33598351999535225, 0.43019292400276754, 0.33423777800635435, 0.32622772399918176, 0.3253342769894516, 0.3248865019995719, 0.324618320009904], [0.6332067839975934, 0.3885271559993271, 0.4376835009898059, 0.335983370008762, 0.4301903049927205, 0.33423778699943796, 0.3262298370100325, 0.32532963599078357, 0.32489187001192477, 0.32462207599019166], [0.3874256540002534, 0.4376850620028563, 0.33598396499291994, 0.315909819008084], [0.3874361150083132, 0.43768469599308446, 0.3359781360049965, 0.4301247209950816, 0.33424936499795876, 0.32622464100131765, 0.32532621099380776, 0.32489784101198893, 0.32461980999505613], [0.3880982559930999, 0.43702299799770117, 0.33598000300116837, 0.43005798500962555, 0.33423127999412827, 0.32623943299404345, 0.32532904999970924, 0.32488991400168743, 0.324631634997786], [0.38808911500382237, 0.43702851999842096, 0.33598358000745066, 0.315704240987543], [0.7659935589908855], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [0.9418128279939992, 0.3885510569962207, 0.43786400101089384, 0.33619083499070257, 0.43017237400636077, 0.3343307659961283, 0.3263375679962337, 0.32539996001287363, 0.32487271098943893, 0.32178805900912266], [], [], [], [], [], [], [0.9418210280127823, 0.3885651789896656, 0.437861170998076, 0.3361614130117232, 0.4302043139905436, 0.33433883399993647, 0.3263259300001664, 0.32539685801020823, 0.3248822309979005, 0.3217741539992858], [], [], [], [], [], [], [], [0.6373814440012211, 0.3885483780031791, 0.437872039998183, 0.3280478149972623], [0.6374466470006155, 0.38854888500645757, 0.3254666430002544], [], [0.6374397129984573, 0.38854281800740864, 0.437814819990308, 0.3360861700057285, 0.43019217099936213, 0.33436487399740145, 0.32631183799821883, 0.3253934560052585, 0.3248927039967384, 0.32497827999759465], [0.9418359049886931, 0.3885348040057579, 0.43788087599386927, 0.3361735270009376, 0.43019160800031386, 0.3343386580090737, 0.32632566399115603, 0.3253943940071622, 0.32488676499633584, 0.3217678960063495], [0.6368722879997222], [], [0.6373489949910436, 0.38856101100100204, 0.437874773007934, 0.3361502869956894, 0.4301986950013088, 0.33434902600129135, 0.3263256310019642, 0.32538249698700383, 0.3248993550078012, 0.3217636310000671], [0.6374616910034092, 0.38854302500840276, 0.4378171939897584, 0.3360832140024286, 0.43019485200056806, 0.3343644510023296, 0.3188642199966125], [0.637461906997487, 0.38853739299520385, 0.3252075360069284], [0.6374043999967398, 0.3885538460017415, 0.43786979500146117, 0.3360905399895273, 0.43019705200276803, 0.33434391699847765, 0.3263273490010761, 0.3253973219980253, 0.324887299007969, 0.321760203994927], [0.6374233730020933, 0.3885509920073673, 0.4378740289976122, 0.3360873920028098, 0.43019676399126183, 0.33436339200125076, 0.3263086990045849, 0.3253929859929485, 0.3248917780001648, 0.3249618990084855], [0.6374684210022679, 0.3885272810002789, 0.4377501170092728, 0.33608748599363025, 0.4301957610005047, 0.33435890699911397, 0.3262470509944251, 0.3253955770051107, 0.3248921049962519, 0.3249806320091011], [0.6374444239918375, 0.38852272399526555, 0.43774912400112953, 0.33608548701158725, 0.4301990849926369, 0.33435838199511636, 0.3262468900065869, 0.32540868400246836, 0.3248797470005229, 0.3249821859935764], [], [0.6374623829906341, 0.38851952500408515, 0.4377551469951868, 0.33608074999938253, 0.4301976100105094, 0.3343591099983314, 0.32624574699730147, 0.3254120519995922, 0.3248811670055147, 0.31781223499274347], [0.6374604800075758, 0.3885380909923697, 0.4377521910064388, 0.33608568299678154, 0.4301947650092188, 0.3343618079961743, 0.326245342992479, 0.3253946720069507, 0.3248940630001016, 0.3249777500022901], [0.6381705200037686, 0.38780973899702076, 0.32479509800032247], [0.6381914959929418, 0.3877973680064315, 0.4376862499921117, 0.3360816210042685, 0.43019570999604184, 0.334360523003852, 0.3262452509952709, 0.3254152570007136, 0.3248804239992751, 0.3249172659998294], [], [], [], [], [], [], [], [], [], [], []], "generated_texts": ["", "", " and understand. Also, you need to add some new information to the \"How to use\" part. Please see the attached file for more details.\nI can help you with reformatting the Filters part and adding new information to the \"How to use\" part. Please provide the attached file so I can review the details and get started on the \"second draft\". I'll make sure to make it easy to read and understand. Let's get started!", "", "\n\n\n\n**Project Charter Template**\n\n**Project Title:** [Insert Project Title]\n\n**Project Sponsor:** [Insert Project Sponsor's Name]\n\n**Project Manager:** [Insert Project Manager's Name]\n\n**Date:** [Insert Date]\n\n**Project Overview:**\n\nThe purpose of this project is to [insert brief project description]. The project aims to [insert project objectives].\n\n**Project Scope:**\n\nThe scope of this project includes:\n\n* [Insert scope statement]\n\n**Project Objectives:**\n\nThe objectives of this project are", "", "", "", " is a simple, open-source", " is a noun that refers to a procedure for evaluating or determining the quality of something, such as a person's knowledge or a substance's properties. It can also refer to a series of questions or exercises used to measure a person's skills or abilities. Additionally, test can be used as a synonym for words such as essay, experiment, trial, exam, examination, quiz, and sample.", "", "", "", " According to various sources, including Bible Gateway and YouVersion, the most quoted biblical verse is John 3:16, which states: \u201cFor God so loved the world that he gave his one and only begotten Son,", "", "swift\nextension AppDelegate: GIDSignInDelegate", "", "", "", " \n\nPlease note that you need to create a `Login` component in the `components` directory for this to work. The `Login` component will contain the actual login form and functionality", "", "", " I'll do my best to create a high-quality response. Please provide the prompt or topic you'd like me to write about.", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", " In Canada, visitors without money can access emergency medical care, but may be billed later. Some provinces offer limited free care for non-residents, while others may require payment upfront.", " The boat can carry 8 people at a time.\n## Step 1: Calculate the total number of trips needed to accommodate all", "", "", "", "\n\n\n\n| Search Intent", "", "", ".\nThe Act also provides that no person shall sell any article of food which is not of the nature, substance, or quality of the article demanded by the purchaser, under a penalty not exceeding \\(\\mathcal{E}50\\).\nThe Act also provides that no person shall sell any article of food which is", " req.params.user;\n res.json({ answers });\n});\n\n// the answers variable should be like this: {\"0523551449", "", "", "", "", "", " The answer, of course, is zero miles. But if you're looking for a more interesting answer, you might consider the distance from Boston to other cities called Boston. There are several of them in the United States, including Boston, Georgia; Boston, Indiana; Boston, Kentucky; Boston, Massachusetts (the one most people think of); Boston, New York; and Boston, Virginia. The distances from Boston, Massachusetts to these other Bostons are: * Boston, GA: 1", "", " \n\nI understand the instructions. You'd like me to:\n\n1. Receive data for each of the 7", "", "", "", "", "", "", "", "", "", "", "", ""], "errors": ["Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "", "", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n", "Traceback (most recent call last):\n  File \"/vllm-workspace/backend_request_func.py\", line 276, in async_request_openai_completions\n    if data[\"choices\"][0][\"text\"]:\nKeyError: 'choices'\n"]}